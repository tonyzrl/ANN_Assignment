{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1XHEo6ggKmUKrGsiFhSNbWPnIO5ywZueb",
      "authorship_tag": "ABX9TyN4bDehvdHkST3YPQbLOBJK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3: Predicting Mapping Penalties with ANN\n",
        "**Due:** June 5, 2025, 11:59 PM\n",
        "\n",
        "**Author:** Tony Liang\n",
        "\n",
        "**Student Number:** 20990204\n",
        "\n",
        "In this assignment, a feed-forward artificial neural network (ANN) is implemented from scratch to predict the penalty score of a mapping between tasks and employees.\n",
        "\n",
        "In this notebook we will:\n",
        "1. Load the 100 mappings dataset  \n",
        "2. Preprocess & encode into 110-dim vectors  \n",
        "3. Define the ANN architectures and implement forward, backward, updates by hand  \n",
        "5. Train via mini-batch SGD over grid of hyperparameters  \n",
        "6. Produce the eight required comparison plots  \n",
        "7. Export results for report submission  \n",
        "\n"
      ],
      "metadata": {
        "id": "J4kSpcuHFsO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Imports"
      ],
      "metadata": {
        "id": "jL-LqT6-jIt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tonyzrl/ANN_Assignment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxYQ598-ambN",
        "outputId": "496f62a9-5615-4080-a869-613d38c182cf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ANN_Assignment' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Task data: ID, Estimated Time, Difficulty, Deadline, Skill Required\n",
        "tasks = [{\"id\": \"T1\", \"estimated_time\": 4, \"difficulty\": 3, \"deadline\": 8, \"skill_required\": \"A\"},\n",
        "        {\"id\": \"T2\", \"estimated_time\": 6, \"difficulty\": 5, \"deadline\": 12, \"skill_required\": \"B\"},\n",
        "        {\"id\": \"T3\", \"estimated_time\": 2, \"difficulty\": 2, \"deadline\": 6, \"skill_required\": \"A\"},\n",
        "        {\"id\": \"T4\", \"estimated_time\": 5, \"difficulty\": 4, \"deadline\": 10, \"skill_required\": \"C\"},\n",
        "        {\"id\": \"T5\", \"estimated_time\": 3, \"difficulty\": 1, \"deadline\": 7, \"skill_required\": \"A\"},\n",
        "        {\"id\": \"T6\", \"estimated_time\": 8, \"difficulty\": 6, \"deadline\": 15, \"skill_required\": \"B\"},\n",
        "        {\"id\": \"T7\", \"estimated_time\": 4, \"difficulty\": 3, \"deadline\": 9, \"skill_required\": \"C\"},\n",
        "        {\"id\": \"T8\", \"estimated_time\": 7, \"difficulty\": 5, \"deadline\": 14, \"skill_required\": \"B\"},\n",
        "        {\"id\": \"T9\", \"estimated_time\": 2, \"difficulty\": 2, \"deadline\": 5, \"skill_required\": \"A\"},\n",
        "        {\"id\": \"T10\", \"estimated_time\": 6, \"difficulty\": 4, \"deadline\": 11, \"skill_required\": \"C\"},]\n",
        "\n",
        "# Employee data: ID, Available hours, Skill level, Skills\n",
        "employees = [{\"id\": \"E1\", \"hours_avail\": 10, \"skill_level\": 4, \"skills\": [\"A\", \"C\"]},\n",
        "            {\"id\": \"E2\", \"hours_avail\": 12, \"skill_level\": 6, \"skills\": [\"A\", \"B\", \"C\"]},\n",
        "            {\"id\": \"E3\", \"hours_avail\": 8, \"skill_level\": 3, \"skills\": [\"A\"]},\n",
        "            {\"id\": \"E4\", \"hours_avail\": 15, \"skill_level\": 7, \"skills\": [\"B\", \"C\"]},\n",
        "            {\"id\": \"E5\", \"hours_avail\": 9, \"skill_level\": 5, \"skills\": [\"A\", \"C\"]}]"
      ],
      "metadata": {
        "id": "P8Q4Pi2MTDB_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading & Preprocessing"
      ],
      "metadata": {
        "id": "Zh32dB9-jXV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(skills):\n",
        "    \"\"\"\n",
        "    One-hot encode a list of skills, e.g. ['A','C'] -> [1,0,1].\n",
        "    \"\"\"\n",
        "    mapping = {'A': 0, 'B': 1, 'C': 2}\n",
        "    vec = [0, 0, 0]\n",
        "    for s in skills:\n",
        "        vec[mapping[s]] = 1\n",
        "    return vec\n",
        "\n",
        "def construct_input_vector(mapping_row):\n",
        "    \"\"\"\n",
        "    Given one row of the mapping CSV (taskâ†’employee assignments + penalty),\n",
        "    plus the list of task & employee, construct the 110-dim vector.\n",
        "    \"\"\"\n",
        "    input_vector = []\n",
        "    # First 10 entries are employee assignments; last entry is penalty\n",
        "    assignments = mapping_row[:10]\n",
        "\n",
        "    for idx, emp_id in enumerate(assignments, start=1):\n",
        "        task_id = f\"T{idx}\"\n",
        "        # Find the task dict\n",
        "        task = next(t for t in tasks if t[\"id\"] == task_id)\n",
        "        # Find the employee dict\n",
        "        emp = next(e for e in employees if e[\"id\"] == emp_id)\n",
        "\n",
        "        # Task features: [time, difficulty, deadline] + one-hot(required skill)\n",
        "        task_features = [\n",
        "            task[\"estimated_time\"],\n",
        "            task[\"difficulty\"],\n",
        "            task[\"deadline\"]\n",
        "        ] + one_hot_encode(task[\"skill_required\"])\n",
        "\n",
        "        # Employee features: [hours_avail, skill_level] + one-hot(skills)\n",
        "        emp_features = [\n",
        "            emp[\"hours_avail\"],\n",
        "            emp[\"skill_level\"],\n",
        "        ] + one_hot_encode(emp[\"skills\"])\n",
        "\n",
        "        input_vector.extend(task_features + emp_features)\n",
        "\n",
        "    return np.array(input_vector)"
      ],
      "metadata": {
        "id": "grbsbbxLTBfC"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/ANN_Assignment/data/task_assignment_data.csv')\n",
        "data = data.values\n",
        "\n",
        "assignment_inputs = []\n",
        "penalties = []\n",
        "\n",
        "for row in data:\n",
        "    assignment_inputs.append(construct_input_vector(row))\n",
        "    penalties.append(row[-1])\n",
        "\n",
        "X = np.vstack(assignment_inputs)             # (N, 110)\n",
        "Y = np.array(penalties).reshape(-1, 1) # (N,   1)\n",
        "\n",
        "# Shuffle data and split 70/15/15\n",
        "N = X.shape[0]\n",
        "perm = np.random.permutation(N)\n",
        "X, Y = X[perm], Y[perm]\n",
        "\n",
        "X_train, Y_train = X[:70], Y[:70]\n",
        "X_val,   Y_val   = X[70:85], Y[70:85]\n",
        "X_test,  Y_test  = X[85:],   Y[85:]\n",
        "\n",
        "# Transpose for Network\n",
        "X_train, Y_train = X_train.T, Y_train.T   # (110, N_train), (1, N_train)\n",
        "X_val, Y_val = X_val.T, Y_val.T     # (110, N_val),   (1, N_val)\n",
        "X_test,  Y_test  = X_test.T,  Y_test.T    # (110, N_test),  (1, N_test)\n",
        "\n",
        "print(\"Shapes:\", X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "id": "AsqUoDkY74U_",
        "outputId": "64dc2045-af4c-4354-a8f0-7ecac66bf29b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (110, 70) (1, 70) (110, 15) (1, 15) (110, 15) (1, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definitions"
      ],
      "metadata": {
        "id": "yAAYSJx8j84Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReLU**\n",
        "\n",
        "The Rectified Linear Unit (ReLU) is a simple, yet highly effective activation function commonly used in Neural Networks. It is defined as:\n",
        "\n",
        "**\\begin{equation}\n",
        "f(Z) = max(0, Z)\n",
        "\\end{equation}**\n",
        "\n",
        "Where $Z$ is the input to the function. ReLU sets all negative values of $Z$ to zero, and leaves the positive values unchanged.\n",
        "\n",
        "The derivative of the ReLU function can be computed as:\n",
        "\n",
        "**\\begin{equation}\n",
        "f'(Z) = \\begin{cases}\n",
        "0, & \\text{if } Z \\leq 0 \\\n",
        "1, & \\text{if } Z > 0\n",
        "\\end{cases}\n",
        "\\end{equation}**\n",
        "\n",
        "---\n",
        "\n",
        "**Sigmoid**\n",
        "\n",
        "The Sigmoid function is a common activation function used in Neural Networks, particularly for binary classification problems. It is represented by the following formula:\n",
        "\n",
        "**\\begin{equation}\n",
        "f(Z) = \\frac{1}{1+e^{-Z}}\n",
        "\\end{equation}**\n",
        "\n",
        "Where $Z$ is the input to the function. The Sigmoid function maps any real-valued number to a value between 0 and 1, which can be interpreted as a probability.\n",
        "\n",
        "The derivative of the Sigmoid function can be computed as:\n",
        "\n",
        "\\begin{equation}\n",
        "f'(Z) = f(Z)(1-f(Z))\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "sAHNifkWOi5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implement the Sigmoid function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter\n",
        "    cache -- a python dictionary containing \"A\" for backpropagation\n",
        "    \"\"\"\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def sigmoid_deriv(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single sigmoid unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient\n",
        "    cache -- 'Z' stored during forward pass\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    Z = cache\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    return dZ\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the ReLU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter\n",
        "    cache -- used for backpropagation\n",
        "    \"\"\"\n",
        "    A = np.maximum(0,Z)\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu_deriv(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single ReLU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient\n",
        "    cache -- 'Z'  stored for backpropagation\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    # When z <= 0, dz is equal to 0 as well.\n",
        "    dZ[Z <= 0] = 0\n",
        "\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "lmoqmu8EIW03"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yHdoZ-1mSME4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_dims=[110, 256, 1], learning_rate=1e-5, activation='relu'):\n",
        "        \"\"\"\n",
        "        layer_dims: list of layer sizes, e.g [110,256,1] or [110,128,128,1]\n",
        "        learning_rate: step size for gradient descent\n",
        "        activation: 'relu' or 'sigmoid' for HIDDEN layers\n",
        "        \"\"\"\n",
        "        self.layer_dims    = layer_dims\n",
        "        self.learning_rate = learning_rate\n",
        "        # pick activation & its derivative\n",
        "        if activation.lower() == 'relu':\n",
        "            self.activation       = relu\n",
        "            self.activation_deriv = relu_deriv\n",
        "        else:\n",
        "            self.activation       = sigmoid\n",
        "            self.activation_deriv = sigmoid_deriv\n",
        "\n",
        "        # number of layers (excluding input)\n",
        "        self.L = len(layer_dims) - 1\n",
        "\n",
        "        # initialise parameters for weights and biases\n",
        "        for l in range(1, self.L + 1):\n",
        "            n_in  = layer_dims[l-1]\n",
        "            n_out = layer_dims[l]\n",
        "            setattr(self, f'W{l}', np.random.randn(n_out, n_in) * 0.01)\n",
        "            setattr(self, f'b{l}', np.zeros((n_out, 1)))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs a full forward pass.\n",
        "        Returns:\n",
        "          Y_hat: (1, m) predictions\n",
        "          caches: list of ((A_prev,W,b), Z) tuples\n",
        "        \"\"\"\n",
        "        caches = []\n",
        "        A = X\n",
        "        # hidden layers\n",
        "        for l in range(1, self.L):\n",
        "            W = getattr(self, f'W{l}')\n",
        "            b = getattr(self, f'b{l}')\n",
        "            Z = W @ A + b\n",
        "            A, _cacheZ = self.activation(Z)      # returns (A, Z)\n",
        "            caches.append(((A, W, b), _cacheZ))\n",
        "        # output layer (linear)\n",
        "        Wl = getattr(self, f'W{self.L}')\n",
        "        bl = getattr(self, f'b{self.L}')\n",
        "        ZL = Wl @ A + bl\n",
        "        caches.append(((A, Wl, bl), ZL))\n",
        "        return ZL, caches\n",
        "\n",
        "    def compute_cost(self, Y_hat, Y):\n",
        "        \"\"\"\n",
        "        Mean Squared Error:\n",
        "          (1/m) * sum((Y_hat - Y)^2)\n",
        "        Y_hat, Y both shape (1, m)\n",
        "        \"\"\"\n",
        "        return np.mean((Y_hat - Y)**2)\n",
        "\n",
        "    def back_layer(self, dZ, cache):\n",
        "        \"\"\"\n",
        "        Backprop for a single layer given dZ = dL/dZ_l.\n",
        "        cache: ((A_prev, W, b), Z)\n",
        "        Returns dA_prev, dW, db.\n",
        "        \"\"\"\n",
        "        (A_prev, W, b), Z = cache\n",
        "        m = A_prev.shape[1]\n",
        "        dW = (1/m) * (dZ @ A_prev.T)\n",
        "        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dA_prev = W.T @ dZ\n",
        "        return dA_prev, dW, db\n",
        "\n",
        "    def backward(self, Y_hat, Y, caches):\n",
        "        \"\"\"\n",
        "        Performs backprop over the whole network.\n",
        "        Returns a dict of grads {dW1, db1, â€¦, dWL, dbL}.\n",
        "        \"\"\"\n",
        "        grads = {}\n",
        "        m = Y.shape[1]\n",
        "\n",
        "        # dZ for MSE loss at output: d/dZ [ (1/m) âˆ‘ (ZL - Y)^2 ] = 2*(ZL - Y)/m\n",
        "        dZ = 2 * (Y_hat - Y) / m\n",
        "\n",
        "        # **Output layer** gradients\n",
        "        cacheL = caches[-1]\n",
        "        dA_prev, dWl, dbl = self.back_layer(dZ, cacheL)\n",
        "        grads[f'dW{self.L}'] = dWl\n",
        "        grads[f'db{self.L}'] = dbl\n",
        "\n",
        "        # **Hidden layers** (L-1 .. 1)\n",
        "        dA = dA_prev\n",
        "        for l in reversed(range(1, self.L)):\n",
        "            cache_l = caches[l-1]\n",
        "            # first convert dA â†’ dZ via activation derivative\n",
        "            Z = cache_l[1]  # this is the stored Z\n",
        "            dZ = self.activation_deriv(dA, Z)\n",
        "            dA, dW, db = self.back_layer(dZ, cache_l)\n",
        "            grads[f'dW{l}'] = dW\n",
        "            grads[f'db{l}'] = db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_parameters(self, grads):\n",
        "        \"\"\"\n",
        "        Applies gradient descent: W -= lr * dW,  b -= lr * db.\n",
        "        \"\"\"\n",
        "        for l in range(1, self.L+1):\n",
        "            W = getattr(self, f'W{l}')\n",
        "            b = getattr(self, f'b{l}')\n",
        "            dW = grads[f'dW{l}']\n",
        "            db = grads[f'db{l}']\n",
        "            setattr(self, f'W{l}', W - self.learning_rate * dW)\n",
        "            setattr(self, f'b{l}', b - self.learning_rate * db)"
      ],
      "metadata": {
        "id": "IGucjLhqj_R5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "QAWLQH0dkKkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(layer_dims, X_train, y_train,\n",
        "               X_val, y_val, X_test, y_test,\n",
        "               learning_rates, batch_sizes,\n",
        "               activations, epochs):\n",
        "    \"\"\"\n",
        "    Train a NeuralNetwork with grid search over learning_rates, batch_sizes, activations.\n",
        "\n",
        "    Returns:\n",
        "      results: list of dicts with keys\n",
        "        'learning_rate', 'batch_size', 'activation',\n",
        "        'train_losses', 'val_losses', 'epoch_times', 'test_loss'\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for batch_size in batch_sizes:\n",
        "            for activation in activations:\n",
        "                # instantiate a fresh model for this config\n",
        "                model = NeuralNetwork(layer_dims, activation=activation)\n",
        "\n",
        "                train_losses = []\n",
        "                val_losses   = []\n",
        "                epoch_times  = []\n",
        "\n",
        "                for epoch in range(epochs):\n",
        "                    t0 = time.time()\n",
        "\n",
        "                    # shuffle training examples\n",
        "                    perm = np.random.permutation(X_train.shape[1])\n",
        "                    X_sh, y_sh = X_train[:, perm], y_train[:, perm]\n",
        "\n",
        "                    # mini-batch gradient descent\n",
        "                    for i in range(0, X_sh.shape[1], batch_size):\n",
        "                        xb = X_sh[:, i:i+batch_size]  # shape (110, batch_size)\n",
        "                        yb = y_sh[:, i:i+batch_size]  # shape (1,   batch_size)\n",
        "\n",
        "                        # forward / backward / update\n",
        "                        y_pred, cache = model.forward(xb)\n",
        "                        grads = model.backward(yb, cache)\n",
        "                        model.update_params(grads, lr)\n",
        "\n",
        "                    # record epoch losses & time\n",
        "                    y_tr, _ = model.forward(X_train)\n",
        "                    y_va, _ = model.forward(X_val)\n",
        "                    train_losses.append(np.mean((y_tr - y_train)**2))\n",
        "                    val_losses.append( np.mean((y_va - y_val)**2))\n",
        "                    epoch_times.append(time.time() - t0)\n",
        "\n",
        "                # final test-set evaluation\n",
        "                y_te, _  = model.forward(X_test)\n",
        "                test_loss = np.mean((y_te - y_test)**2)\n",
        "\n",
        "                # store this configurationâ€™s results\n",
        "                results.append({\n",
        "                    'learning_rate': lr,\n",
        "                    'batch_size':   batch_size,\n",
        "                    'activation':   activation,\n",
        "                    'train_losses': train_losses,\n",
        "                    'val_losses':   val_losses,\n",
        "                    'epoch_times':  epoch_times,\n",
        "                    'test_loss':    test_loss\n",
        "                })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# â€”â€”â€” Example usage â€”â€”â€”\n",
        "layer_dims_A = [110, 256, 1]   # Model A\n",
        "layer_dims_B = [110, 128, 128, 1]  # Model B\n",
        "\n",
        "# Hyperparameter grid\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "batch_sizes    = [8, 16, 32]\n",
        "activations    = ['sigmoid', 'relu']\n",
        "epochs         = 100  # or as required\n",
        "\n",
        "results_A = training(layer_dims_A,\n",
        "                       X_train, Y_train,\n",
        "                       X_val,   Y_val,\n",
        "                       X_test,  Y_test,\n",
        "                       learning_rates,\n",
        "                       batch_sizes,\n",
        "                       activations,\n",
        "                       epochs)\n",
        "\n",
        "print(\"Example result:\", results_A[0])"
      ],
      "metadata": {
        "id": "_IVnzs7f6yED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c69a05b-facc-45e9-a70f-6cd57b4ee357"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-132ddff5806d>:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n",
            "<ipython-input-66-132ddff5806d>:5: RuntimeWarning: overflow encountered in multiply\n",
            "  return a * (1 - a)\n",
            "<ipython-input-67-ef984576ef02>:58: RuntimeWarning: overflow encountered in matmul\n",
            "  grads[f'dW{l}'] = dZ @ A_prev.T\n",
            "<ipython-input-67-ef984576ef02>:58: RuntimeWarning: invalid value encountered in matmul\n",
            "  grads[f'dW{l}'] = dZ @ A_prev.T\n",
            "<ipython-input-67-ef984576ef02>:34: RuntimeWarning: invalid value encountered in matmul\n",
            "  Z = self.params[f'W{l}'] @ A_prev + self.params[f'b{l}']\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example result: {'learning_rate': 0.01, 'batch_size': 8, 'activation': 'sigmoid', 'train_losses': [np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan)], 'val_losses': [np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan)], 'epoch_times': [0.010413169860839844, 0.002727508544921875, 0.0029077529907226562, 0.002674102783203125, 0.0025353431701660156, 0.002969503402709961, 0.0028197765350341797, 0.0026497840881347656, 0.002596139907836914, 0.002691030502319336, 0.0024030208587646484, 0.002530336380004883, 0.0028030872344970703, 0.0029244422912597656, 0.0028412342071533203, 0.0029277801513671875, 0.002909421920776367, 0.002685546875, 0.0026400089263916016, 0.0031168460845947266, 0.0030303001403808594, 0.00590205192565918, 0.002843141555786133, 0.0028045177459716797, 0.0028100013732910156, 0.002604246139526367, 0.002790689468383789, 0.0026607513427734375, 0.002676248550415039, 0.003027677536010742, 0.0028619766235351562, 0.002713918685913086, 0.002755403518676758, 0.002864360809326172, 0.002840280532836914, 0.002486705780029297, 0.0025446414947509766, 0.002850770950317383, 0.002470731735229492, 0.002745389938354492, 0.0028710365295410156, 0.0029332637786865234, 0.0025801658630371094, 0.0027844905853271484, 0.002954244613647461, 0.0024700164794921875, 0.002423524856567383, 0.002445697784423828, 0.002777099609375, 0.00269317626953125, 0.002615690231323242, 0.0026886463165283203, 0.0028548240661621094, 0.002363443374633789, 0.0024535655975341797, 0.0026214122772216797, 0.0028028488159179688, 0.0026192665100097656, 0.0026650428771972656, 0.002804994583129883, 0.002404451370239258, 0.0023469924926757812, 0.0025756359100341797, 0.0029871463775634766, 0.0028514862060546875, 0.0025599002838134766, 0.002530336380004883, 0.002907276153564453, 0.0024912357330322266, 0.0024242401123046875, 0.002572774887084961, 0.005046844482421875, 0.002485036849975586, 0.0025968551635742188, 0.002925872802734375, 0.002639293670654297, 0.002700328826904297, 0.0028913021087646484, 0.0029268264770507812, 0.0027229785919189453, 0.005486011505126953, 0.002768993377685547, 0.002681732177734375, 0.0027418136596679688, 0.0029468536376953125, 0.0027146339416503906, 0.002539396286010742, 0.0028924942016601562, 0.002624988555908203, 0.002544879913330078, 0.0026581287384033203, 0.0029397010803222656, 0.002797842025756836, 0.002667665481567383, 0.0028231143951416016, 0.0027196407318115234, 0.0024950504302978516, 0.002451658248901367, 0.002701997756958008, 0.0028705596923828125], 'test_loss': np.float64(nan)}\n"
          ]
        }
      ]
    }
  ]
}