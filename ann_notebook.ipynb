{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1XHEo6ggKmUKrGsiFhSNbWPnIO5ywZueb",
      "authorship_tag": "ABX9TyMml0yiJDyweQDYJhVG9kYy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3: Predicting Mapping Penalties with ANN\n",
        "**Due:** June 5, 2025, 11:59 PM\n",
        "\n",
        "**Author:** Tony Liang\n",
        "\n",
        "**Student Number:** 20990204\n",
        "\n",
        "In this assignment, a feed-forward artificial neural network (ANN) is implemented from scratch to predict the penalty score of a mapping between tasks and employees.\n",
        "\n",
        "In this notebook we will:\n",
        "1. Load the 100 mappings dataset  \n",
        "2. Preprocess & encode into 110-dim vectors  \n",
        "3. Define two ANN architectures (Model A & Model B)  \n",
        "4. Implement forward, backward, updates by hand  \n",
        "5. Train via mini-batch SGD over grid of hyperparameters  \n",
        "6. Produce the eight required comparison plots  \n",
        "7. Export results for report submission  \n",
        "\n"
      ],
      "metadata": {
        "id": "J4kSpcuHFsO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Imports"
      ],
      "metadata": {
        "id": "jL-LqT6-jIt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib . pyplot as plt\n",
        "import time\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "!git clone https://github.com/tonyzrl/ANN_Assignment\n",
        "\n",
        "# Task data: ID, Estimated Time, Difficulty, Deadline, Skill Required\n",
        "tasks = [{\"id\": \"T1\", \"estimated_time\": 4, \"difficulty\": 3, \"deadline\": 8, \"skill_required\": \"A\"},\n",
        "        {\"id\": \"T2\", \"estimated_time\": 6, \"difficulty\": 5, \"deadline\": 12, \"skill_required\": \"B\"},\n",
        "        {\"id\": \"T3\", \"estimated_time\": 2, \"difficulty\": 2, \"deadline\": 6, \"skill_required\": \"A\"},\n",
        "        {\"id\": \"T4\", \"estimated_time\": 5, \"difficulty\": 4, \"deadline\": 10, \"skill_required\": \"C\"},\n",
        "        {\"id\": \"T5\", \"estimated_time\": 3, \"difficulty\": 1, \"deadline\": 7, \"skill_required\": \"A\"},\n",
        "        {\"id\": \"T6\", \"estimated_time\": 8, \"difficulty\": 6, \"deadline\": 15, \"skill_required\": \"B\"},\n",
        "        {\"id\": \"T7\", \"estimated_time\": 4, \"difficulty\": 3, \"deadline\": 9, \"skill_required\": \"C\"},\n",
        "        {\"id\": \"T8\", \"estimated_time\": 7, \"difficulty\": 5, \"deadline\": 14, \"skill_required\": \"B\"},\n",
        "        {\"id\": \"T9\", \"estimated_time\": 2, \"difficulty\": 2, \"deadline\": 5, \"skill_required\": \"A\"},\n",
        "        {\"id\": \"T10\", \"estimated_time\": 6, \"difficulty\": 4, \"deadline\": 11, \"skill_required\": \"C\"},]\n",
        "\n",
        "# Employee data: ID, Available hours, Skill level, Skills\n",
        "employees = [{\"id\": \"E1\", \"hours_avail\": 10, \"skill_level\": 4, \"skills\": [\"A\", \"C\"]},\n",
        "            {\"id\": \"E2\", \"hours_avail\": 12, \"skill_level\": 6, \"skills\": [\"A\", \"B\", \"C\"]},\n",
        "            {\"id\": \"E3\", \"hours_avail\": 8, \"skill_level\": 3, \"skills\": [\"A\"]},\n",
        "            {\"id\": \"E4\", \"hours_avail\": 15, \"skill_level\": 7, \"skills\": [\"B\", \"C\"]},\n",
        "            {\"id\": \"E5\", \"hours_avail\": 9, \"skill_level\": 5, \"skills\": [\"A\", \"C\"]}]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Q4Pi2MTDB_",
        "outputId": "73a1e67f-ed6f-4b53-cf4b-e27d87ec4d38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ANN_Assignment'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 46 (delta 11), reused 25 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (46/46), 17.31 KiB | 17.31 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading & Preprocessing"
      ],
      "metadata": {
        "id": "Zh32dB9-jXV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/ANN_Assignment/data/task_assignment_data.csv')\n",
        "data = df.values\n",
        "\n",
        "def one_hot_encode(skills):\n",
        "    \"\"\"\n",
        "    One-hot encode a list of skills, e.g. ['A','C'] -> [1,0,1].\n",
        "    \"\"\"\n",
        "    mapping = {'A': 0, 'B': 1, 'C': 2}\n",
        "    vec = [0, 0, 0]\n",
        "    for s in skills:\n",
        "        vec[mapping[s]] = 1\n",
        "    return vec\n",
        "\n",
        "def construct_input_vector(mapping_row):\n",
        "    \"\"\"\n",
        "    Given one row of the mapping CSV (task→employee assignments + penalty),\n",
        "    plus the list of task & employee, construct the 110-dim vector.\n",
        "    \"\"\"\n",
        "    input_vector = []\n",
        "    # First 10 entries are employee assignments; last entry is penalty\n",
        "    assignments = mapping_row[:10]\n",
        "\n",
        "    for idx, emp_id in enumerate(assignments, start=1):\n",
        "        task_id = f\"T{idx}\"\n",
        "        # Find the task dict\n",
        "        task = next(t for t in tasks if t[\"id\"] == task_id)\n",
        "        # Find the employee dict\n",
        "        emp = next(e for e in employees if e[\"id\"] == emp_id)\n",
        "\n",
        "        # Task features: [time, difficulty, deadline] + one-hot(required skill)\n",
        "        task_features = [\n",
        "            task[\"estimated_time\"],\n",
        "            task[\"difficulty\"],\n",
        "            task[\"deadline\"]\n",
        "        ] + one_hot_encode(task[\"skill_required\"])\n",
        "\n",
        "        # Employee features: [hours_avail, skill_level] + one-hot(skills)\n",
        "        emp_features = [\n",
        "            emp[\"hours_avail\"],\n",
        "            emp[\"skill_level\"],\n",
        "        ] + one_hot_encode(emp[\"skills\"])\n",
        "\n",
        "        input_vector.extend(task_features + emp_features)\n",
        "\n",
        "    return np.array(input_vector)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Processing Data\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "assignments = data[:, :10]      # shape (m, 10) of employee IDs\n",
        "penalties   = data[:, 10]       # shape (m,)\n",
        "\n",
        "# 2. Vectorize each row into a 110-dim feature vector\n",
        "X_raw = np.array([\n",
        "    construct_input_vector(list(row))[0]\n",
        "    for row in data\n",
        "])                                # shape (m, 110)\n",
        "y_raw = penalties.reshape(-1, 1) # shape (m, 1)\n",
        "\n",
        "# 4. Transpose for Lab9 convention: columns are examples\n",
        "X = X_raw.T  # now (110, m)\n",
        "y = y_raw.T  # now (1,   m)\n",
        "\n",
        "# 5. Shuffle\n",
        "perm = np.random.permutation(X.shape[1])\n",
        "X, y = X[:, perm], y[:, perm]\n",
        "\n",
        "# 6. Split 70/15/15\n",
        "m       = X.shape[1]\n",
        "n_train = int(0.7 * m)\n",
        "n_val   = int(0.15 * m)\n",
        "\n",
        "X_train, y_train = X[:, :n_train],          y[:, :n_train]\n",
        "X_val,   y_val   = X[:, n_train:n_train+n_val], y[:, n_train:n_train+n_val]\n",
        "X_test,  y_test  = X[:, n_train+n_val:],     y[:, n_train+n_val:]"
      ],
      "metadata": {
        "id": "grbsbbxLTBfC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = [\"E2\",\"E3\",\"E3\",\"E2\",\"E2\",\"E2\",\"E1\",\"E5\",\"E1\",\"E5\",4.6000000000000005]\n",
        "input = construct_input_vector(vector)\n",
        "print(input)\n",
        "print(input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV8AOG0ArspN",
        "outputId": "cf0a58b6-e62d-4e22-cf89-22dd5a25e21c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4  3  8  1  0  0 12  6  1  1  1  6  5 12  0  1  0  8  3  1  0  0  2  2\n",
            "  6  1  0  0  8  3  1  0  0  5  4 10  0  0  1 12  6  1  1  1  3  1  7  1\n",
            "  0  0 12  6  1  1  1  8  6 15  0  1  0 12  6  1  1  1  4  3  9  0  0  1\n",
            " 10  4  1  0  1  7  5 14  0  1  0  9  5  1  0  1  2  2  5  1  0  0 10  4\n",
            "  1  0  1  6  4 11  0  0  1  9  5  1  0  1]\n",
            "(110,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions"
      ],
      "metadata": {
        "id": "D3wuDaBFo4zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_deriv(a):\n",
        "    return a * (1 - a)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_deriv(z):\n",
        "    return (z > 0).astype(float)"
      ],
      "metadata": {
        "id": "mjbDT4Bvo3_y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model A Definitions"
      ],
      "metadata": {
        "id": "yAAYSJx8j84Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkA :\n",
        "  def __init__ (self, activation =(\"relu\")):\n",
        "\n",
        "    # Define activation function\n",
        "    if activation == 'relu':\n",
        "        self.act_func, self.act_func_deriv = relu, relu_deriv\n",
        "    else:\n",
        "        self.act_func, self.act_prime_deriv = sigmoid, sigmoid_deriv\n",
        "\n",
        "    # Layer params\n",
        "    self.W1 = np.random.randn(256, 110) * 0.01\n",
        "    self.b1 = np.zeros((256, 1))\n",
        "    self.W2 = np.random.randn(1, 256) * 0.01\n",
        "    self.b2 = np.zeros((1, 1))\n",
        "\n",
        "  def forward(self , x):\n",
        "    \"\"\"\n",
        "    x: (110, m)  # columns are examples\n",
        "    returns: A2 (1, m) and cache for backprop\n",
        "    \"\"\"\n",
        "    # Layer 1\n",
        "    Z1 = self.W1.dot(x) + self.b1    # (256, m)\n",
        "    A1 = self.act_func(Z1)                # (256, m)\n",
        "    # Layer 2 (output, linear)\n",
        "    Z2 = self.W2.dot(A1) + self.b2   # (1, m)\n",
        "    A2 = Z2                          # identity\n",
        "\n",
        "    cache = (x, Z1, A1, Z2, A2)\n",
        "    return A2, cache\n",
        "\n",
        "  def backward(self, y_true, cache):\n",
        "    \"\"\"\n",
        "    y_true: (1, m) true values\n",
        "    cache: x, Z1, A1, Z2, A2\n",
        "    returns: grads dict {dW1, db1, dW2, db2}\n",
        "    \"\"\"\n",
        "    x, Z1, A1, Z2, A2 = cache\n",
        "    m = x.shape[1]\n",
        "\n",
        "    # output layer gradient (MSE)\n",
        "    dZ2 = 2 * (A2 - y_true) / m            # (1, m)\n",
        "    dW2 = dZ2.dot(A1.T)               # (1, 256)\n",
        "    db2 = np.sum(dZ2, axis=1, keepdims=True)  # (1,1)\n",
        "\n",
        "    # hidden layer gradient\n",
        "    dA1 = self.W2.T.dot(dZ2)          # (256, m)\n",
        "    dZ1 = dA1 * self.act_func_deriv(Z1)    # (256, m)\n",
        "    dW1 = dZ1.dot(x.T)                # (256, 110)\n",
        "    db1 = np.sum(dZ1, axis=1, keepdims=True)  # (256,1)\n",
        "\n",
        "    grads = {'dW1':dW1, 'db1':db1, 'dW2':dW2, 'db2':db2}\n",
        "    return grads\n",
        "\n",
        "  def update_params (self, grads, lr):\n",
        "    self.W1 -= lr * grads['dW1']\n",
        "    self.b1 -= lr * grads['db1']\n",
        "    self.W2 -= lr * grads['dW2']\n",
        "    self.b2 -= lr * grads['db2']\n"
      ],
      "metadata": {
        "id": "IGucjLhqj_R5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model B Definitions\n"
      ],
      "metadata": {
        "id": "9mXjb4GHovv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkB:\n",
        "    def __init__(self, activation='relu'):\n",
        "\n",
        "        # Define activation function\n",
        "        if activation == 'relu':\n",
        "            self.act_func, self.act_func_deriv = relu, relu_deriv\n",
        "        else:\n",
        "            self.act_func, self.act_func_deriv = sigmoid, sigmoid_deriv\n",
        "\n",
        "        # Layer params\n",
        "        self.W1 = np.random.randn(128, 110) * 0.01\n",
        "        self.b1 = np.zeros((128, 1))\n",
        "        self.W2 = np.random.randn(128, 128) * 0.01\n",
        "        self.b2 = np.zeros((128, 1))\n",
        "        self.W3 = np.random.randn(1, 128) * 0.01\n",
        "        self.b3 = np.zeros((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (110, m)\n",
        "        returns: A3 (1, m) and cache\n",
        "        \"\"\"\n",
        "        Z1 = self.W1.dot(x) + self.b1      # (128, m)\n",
        "        A1 = self.act_func(Z1)                  # (128, m)\n",
        "        Z2 = self.W2.dot(A1) + self.b2     # (128, m)\n",
        "        A2 = self.act_func(Z2)                  # (128, m)\n",
        "        Z3 = self.W3.dot(A2) + self.b3     # (1, m)\n",
        "        A3 = Z3                            # identity\n",
        "\n",
        "        cache = (x, Z1, A1, Z2, A2, Z3, A3)\n",
        "        return A3, cache\n",
        "\n",
        "    def backward(self, y_true, cache):\n",
        "        \"\"\"\n",
        "        y_true: (1, m)\n",
        "        cache: X, Z1, A1, Z2, A2, Z3, A3\n",
        "        returns: grads dict\n",
        "        \"\"\"\n",
        "        x, Z1, A1, Z2, A2, Z3, A3 = cache\n",
        "        m = x.shape[1]\n",
        "\n",
        "        # output layer\n",
        "        dZ3 = 2 * (A3 - y_true) / m              # (1, m)\n",
        "        dW3 = dZ3.dot(A2.T)                 # (1,128)\n",
        "        db3 = np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "        # hidden layer 2\n",
        "        dA2 = self.W3.T.dot(dZ3)            # (128, m)\n",
        "        dZ2 = dA2 * self.act_func_deriv(Z2)      # (128, m)\n",
        "        dW2 = dZ2.dot(A1.T)                 # (128,128)\n",
        "        db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "        # hidden layer 1\n",
        "        dA1 = self.W2.T.dot(dZ2)            # (128, m)\n",
        "        dZ1 = dA1 * self.act_func_deriv(Z1)      # (128, m)\n",
        "        dW1 = dZ1.dot(x.T)                  # (128,110)\n",
        "        db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        return {'dW1':dW1,'db1':db1,'dW2':dW2,'db2':db2,'dW3':dW3,'db3':db3}\n",
        "\n",
        "    def update_params(self, grads, lr):\n",
        "        self.W1 -= lr * grads['dW1']\n",
        "        self.b1 -= lr * grads['db1']\n",
        "        self.W2 -= lr * grads['dW2']\n",
        "        self.b2 -= lr * grads['db2']\n",
        "        self.W3 -= lr * grads['dW3']\n",
        "        self.b3 -= lr * grads['db3']"
      ],
      "metadata": {
        "id": "19M6nDkJo_c3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "QAWLQH0dkKkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter grid\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "batch_sizes    = [8, 16, 32]\n",
        "activations    = ['sigmoid', 'relu']\n",
        "epochs         = 100  # or as required\n",
        "\n",
        "# Placeholder for results\n",
        "results = []\n",
        "\n",
        "# Assume you have:\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test defined,\n",
        "# and a NeuralNetwork class with:\n",
        "#   model = NeuralNetwork([110,256,1], activation=phi)\n",
        "#   y_hat, cache = model.forward(xb)\n",
        "#   grads = model.backward(xb, yb, cache)\n",
        "#   model.update_params(grads, lr)\n",
        "\n",
        "for alpha in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        for func in activations:\n",
        "            # Initialize model for Model A architecture\n",
        "            model = NeuralNetwork([110, 256, 1], activation=func)\n",
        "            train_losses = []\n",
        "            val_losses   = []\n",
        "            epoch_times  = []\n",
        "\n",
        "            # Epoch loop\n",
        "            for e in range(epochs):\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Shuffle training data\n",
        "                perm = np.random.permutation(X_train.shape[0])\n",
        "                X_sh, y_sh = X_train[perm], y_train[perm]\n",
        "\n",
        "                # Mini-batch training\n",
        "                for i in range(0, X_sh.shape[0], batch_size):\n",
        "                    xb = X_sh[i:i+batch_size]\n",
        "                    yb = y_sh[i:i+batch_size]\n",
        "\n",
        "                    # Forward + backward + update\n",
        "                    y_pred, cache = model.forward(xb)\n",
        "                    grads = model.backward(xb, yb, cache)\n",
        "                    model.update_params(grads, alpha)\n",
        "\n",
        "                # End of epoch: measure metrics\n",
        "                y_train_pred, _ = model.forward(X_train)\n",
        "                y_val_pred,   _ = model.forward(X_val)\n",
        "                train_loss = np.mean((y_train_pred - y_train)**2)\n",
        "                val_loss   = np.mean((y_val_pred   - y_val  )**2)\n",
        "\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                epoch_times.append(time.time() - start_time)\n",
        "\n",
        "            # Final test-set evaluation\n",
        "            y_test_pred, _ = model.forward(X_test)\n",
        "            test_loss = np.mean((y_test_pred - y_test)**2)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'learning_rate': alpha,\n",
        "                'batch_size': batch_size,\n",
        "                'activation': phi,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'epoch_times': epoch_times,\n",
        "                'test_loss': test_loss\n",
        "            })\n",
        "\n",
        "# Example: inspect one result entry\n",
        "print(\"Example result:\", results[0])"
      ],
      "metadata": {
        "id": "_IVnzs7f6yED"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}